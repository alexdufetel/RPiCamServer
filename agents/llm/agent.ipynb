{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S38_25YnNXIN"
      },
      "source": [
        "# LLM Agent\n",
        "\n",
        "This runs an agent, which uses an LLM, to complete Kradle challenges (https://app.kradle.ai). Full code [on Github](https://github.com/Kradle-ai/examples/tree/main/agents/llm).\n",
        "\n",
        "First let's import the Kradle SDK:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fbcDcn3S7tJT"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install kradle==0.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-Mx8YWb79_v"
      },
      "outputs": [],
      "source": [
        "from kradle import (\n",
        "    AgentManager,\n",
        "    MinecraftAgent,\n",
        "    JSON_RESPONSE_FORMAT,\n",
        ")\n",
        "from kradle.models import MinecraftEvent, InitParticipantResponse\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKrGKrYMIJdU"
      },
      "source": [
        "Now we prepare all the subprompts we will use to build the master prompt for our LLM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6N1AvqiNU-pY"
      },
      "outputs": [],
      "source": [
        "# MAIN PROMPT - we're telling the LLM that it's controlling a mineflayer bot named $NAME that plays minecraft. the bot can move around, mine, build, and interact with the world. your goal is: $TASK. It is crucial that you achieve this goal.\n",
        "coding_prompt = \"\"\"You are an agent controlling a mineflayer bot named $NAME that plays minecraft. the bot can move around, mine, build, and interact with the world. your goal is: $TASK. It is crucial that you achieve this goal.\n",
        "\n",
        " $CREATIVE_MODE\n",
        "\n",
        " The way you control the bot is by writing javascript codeblocks. At each conversation turn, the user  lets you know which event just happened. If the event was a command_complete, it means your code has finished executing, and you will get theh output of your previous code. You will also receive your position, your inventory, who do you see, etc.\n",
        "\n",
        " You should return a json with two parts: 'code' containing the javascript code you want the bot to execute, and 'message' which will go to the general chat. For example { 'code': 'await skills.goToPlayer(bot, 'zZZn98');', 'message': 'I'm coming!'}. DO NOT RETURN ANYTHING ELSE THAN THIS JSON. THIS IS VERY IMPORTANT.\n",
        "\n",
        " Also, if you leave code empyty, you will not get a callback with a command_complete event. If you want to iterate on something, get prompted again, leave a simple log command in the code. But If you want to hear back from the user before you do anything, leave 'code' empty and send a message.\n",
        "\n",
        " If you provide 'code', the code will be executed and you will receive its output, which will give you an opportunity to iterate on it. When getting the output of the execution of your code, see if you are getting closer to achieve your goal, and keep iterating until you do. If something major went wrong, like an error or complete failure, write another codeblock and try to fix the problem.\n",
        "\n",
        " This is extremely important to me, think step-by-step, take a deep breath and good luck! \\nConversation:\\n\"\"\"\n",
        "\n",
        "# PERSONA PROMPT\n",
        "persona_prompt = \"your specific persona is: \\n$PERSONA \\n\"\n",
        "\n",
        "# SKILLS PROMPT\n",
        "skills_prompt = \"The code is asynchronous and MUST CALL AWAIT for all async function calls. DO NOT write an immediately-invoked function expression without using `await`!! DO NOT WRITE LIKE THIS: '(async () => {console.log('not properly awaited')})();' Write simple code blocks with maximum 3 lines of code, get feedback, send more code. here is the reference of the javascript code syntax that you can use: \\n\\n $CODE_DOCS \\n\"\n",
        "\n",
        "# EXAMPLES PROMPT\n",
        "examples_prompt = \"Here are examples of Conversations: $EXAMPLES. \"\n",
        "\n",
        "# AGENT MODE PROMPT\n",
        "agent_prompt = \"Your bot has the following configuration: $AGENT_MODE. \"\n",
        "\n",
        "# CREATIVE MODE PROMPT\n",
        "creative_mode_prompt = \"You are in creative mode, you don't need to collect blocks, you can place any block where you want, even if they're not in your inventory. do not collect blocks, only use commands to place blocks.\"\n",
        "\n",
        "# CODING EXAMPLES\n",
        "coding_examples = [\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": \"greg: Collect 10 wood\"},\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": { \"code\": \"await skills.collectBlock(bot, 'oak_log', 10);\", \"message\": \"I collected 9 oak logs, what next?\"},\n",
        "        }\n",
        "    ],\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": \"bobby: cook some chicken\"},\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": { \"code\": \"await skills.smeltItem(bot, 'chicken', 8);\", \"message\": \"I cooked 8 chicken.\"},\n",
        "        }\n",
        "    ],\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": \"zZZn98: come here\"},\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": { \"code\": \"await skills.goToPlayer(bot, 'zZZn98');\", \"message\": \"I'm coming!\"},\n",
        "        }\n",
        "    ],\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": \"maya: go to the nearest oak log\"},\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": { \"code\": \"await skills.goToPosition(bot, nearestOakLog.x, nearestOakLog.y, nearestOakLog.z);\", \"message\": \"Here!\"}\n",
        "        },\n",
        "        {\"role\": \"system\", \"content\": \"Arrived at location.\"},\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": \"I found an oak log and I am now standing next to it. What next?\"\n",
        "        }\n",
        "    ],\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": \"234jeb: build a little tower with a torch on the side\"},\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": { \"code\": \"let pos = bot.entity.position;\\nfor (let i = 0; i < 5; i++) {\\n    await skills.placeBlock(bot, 'dirt', pos.x, pos.y + i, pos.z);\\n}\\nawait skills.placeBlock(bot, 'torch', pos.x + 1, pos.y + 4, pos.z, 'side');\\n\", \"message\": \"Successfully placed 5 dirt.\"},\n",
        "        }\n",
        "    ],\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": \"brug: build a dirt house\"},\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\" : { \"code\": \"const position = world.getPosition(bot);\\nconst startX = position.x;\\nconst startY = position.y;\\nconst startZ = position.z;\\nconst width = 7;\\nconst depth = 7;\\nconst height = 4;\\n\\n// Build the walls\\nfor (let x = startX; x < startX + width; x++) {\\n    for (let y = startY; y < startY + height; y++) {\\n        for (let z = startZ; z < startZ + depth; z++) {\\n            if (x === startX || x === startX + width - 1 || y === startY || y === startY + height - 1 || z === startZ || z === startZ + depth - 1) {\\n                await skills.placeBlock(bot, 'oak_planks', x, y, z);  \\n            }\\n       }\\n    }\\n}\\n\", \"message\": \"Successfully placed 5 dirt.\"},\n",
        "        },\n",
        "    ],\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUPFUaj38Msj"
      },
      "source": [
        "Get your API Key from [Kradle](https://app.kradle.ai) and store it as a Google Colab Secret (click the key icon in the left sidebar). Name the secret 'KRADLE_API_KEY'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCzuXADY8Dql"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import userdata\n",
        "    KRADLE_API_KEY=userdata.get('KRADLE_API_KEY')\n",
        "except:\n",
        "    import os\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "    KRADLE_API_KEY = os.getenv('KRADLE_API_KEY')\n",
        "\n",
        "if KRADLE_API_KEY is None:\n",
        "    raise ValueError(\"KRADLE_API_KEY is not set. In Colab, set it as a secret. When running locally, set it as an environment variable.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA9JDEr0TWYF"
      },
      "source": [
        "Personalize your agent\n",
        "1. Give them a personality: this will get injected in the larger prompt we send to the LLM\n",
        "2. Select a model that supports [response_format](https://openrouter.ai/models?fmt=cards&order=newest&supported_parameters=response_format)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iT603u6Ta9q"
      },
      "outputs": [],
      "source": [
        "# let's define a model and persona for our agent\n",
        "PERSONA = \"you are a cool resourceful agent. you really want to achieve the task that has been given to you.\"  # check out some other personas in prompts/config.py\n",
        "# pick a model from https://openrouter.ai/models?fmt=cards&order=newest&supported_parameters=response_format\n",
        "MODEL = \"google/gemini-2.0-flash-001\"\n",
        "\n",
        "# optional:\n",
        "# increase this if you want to slow down the agent\n",
        "DELAY_AFTER_ACTION = 100  # this adds a delay (in milliseconds) after an action is performed. increase this if the agent is too fast or if you want more time to see the agent's actions\n",
        "#the max number of times the LLM will retry to generate a valid response\n",
        "MAX_RETRIES = 3\n",
        "# you can set a custom openrouter key here. if omitted, the Kradle API provides a key with a small amount of credit\n",
        "OPENROUTER_API_KEY = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq3dCJ1E896V"
      },
      "source": [
        "Let's now build your Agent:\n",
        "- We're subclassing MinecraftAgent\n",
        "- username will be the @handle for your agent. pick a fun one!\n",
        "- we're extending two methods\n",
        "  - init_participant: gets called at the beginning of the challenge with all the required information\n",
        "  - on_event: gets call at each selected event. This is where we build a prompt, send it to the LLM, pass back the action to perform on Kradle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6_xmWiS85gY"
      },
      "outputs": [],
      "source": [
        "# this is your agent class. It extends the MinecraftAgent class in the Kradle SDK\n",
        "# you pass this whole class into the AgentManager.serve() below\n",
        "# this lets Kradle manage the lifecycle of this agent.\n",
        "# Kradle can create multiple instances of your agent (eg. adding two of the same agent to a challenge)\n",
        "# each instance of this class is called a 'participant'\n",
        "class LLMBasedAgent(MinecraftAgent):\n",
        "\n",
        "    username = \"llm-agent\"  # this is the username of the agent (eg. kradle.ai/<my-username>/agents/<my-agent-username>)\n",
        "    display_name = \"LLM Agent\"  # this is the display name of the agent\n",
        "    description = \"This is an LLM-based agent that can be used to perform tasks in Minecraft.\"\n",
        "\n",
        "    # this method is called when the session starts\n",
        "    # challenge_info has variables, like challenge_info.task\n",
        "    # use the return statement of this method to send a list of in-game events you wish to listen to.\n",
        "    # these will trigger the on_event() function when they occur\n",
        "    def init_participant(self, challenge_info):\n",
        "\n",
        "        # self.memory is a utility instantiated for you\n",
        "        # that can be used to store and retrieve information\n",
        "        # it persists across the lifecycle of this participant.\n",
        "        # It is an instance of the StandardMemory class in the Kradle SDK\n",
        "\n",
        "        print(f\"Received init_participant call for {self.participant_id} with task: {challenge_info.task}\")\n",
        "\n",
        "        # save the task to memory\n",
        "        self.memory.task = challenge_info.task\n",
        "\n",
        "        # array to store LLM interaction history\n",
        "        self.memory.llm_transcript = []\n",
        "\n",
        "        # array to store in-game chat history\n",
        "        self.memory.game_chat_history = []\n",
        "\n",
        "        # set Minecraft modes (e.g. creative mode, self_preservation, etc)\n",
        "        # TODO: Link to docs to explain more.\n",
        "        self.memory.agent_modes = challenge_info.agent_modes\n",
        "\n",
        "        # dictionaries to store all possible javascript functions\n",
        "        self.memory.js_functions = challenge_info.js_functions\n",
        "\n",
        "        # storing in memory if you're using a Redis Memory, and want to make your agent resilient to restarts\n",
        "        self.memory.persona = PERSONA\n",
        "        self.memory.model = MODEL\n",
        "        self.memory.delay_after_action = DELAY_AFTER_ACTION\n",
        "\n",
        "        # try to find the openrouter api key in the environment variables\n",
        "        if OPENROUTER_API_KEY is not None and len(OPENROUTER_API_KEY) > 20:\n",
        "            self.memory.openrouter_api_key = OPENROUTER_API_KEY\n",
        "        else:\n",
        "            # get the openrouter api key from the kradle API\n",
        "            human = self._internal_api_client.humans.get()\n",
        "            self.memory.openrouter_api_key = human[\"openRouterKey\"]\n",
        "\n",
        "        print(f\"Initializing agent for participant ID: {self.participant_id} with username: {self.username}\")\n",
        "        print(f\"Persona: {self.memory.persona}\")\n",
        "        print(f\"Model: {self.memory.model}\")\n",
        "        print(f\"Delay after action: {self.memory.delay_after_action}\")\n",
        "        print(f\"agent modes: {self.memory.agent_modes}\")\n",
        "\n",
        "        # self.log() lets us log information to the Kradle dashboard (left pane in the session viewer)\n",
        "        self.log(\n",
        "            {\n",
        "                \"persona\": self.memory.persona,\n",
        "                \"model\": self.memory.model,\n",
        "                \"delay_after_action\": self.memory.delay_after_action,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # tell Kradle what we want to listen to\n",
        "\n",
        "        return InitParticipantResponse({\"listenTo\": [MinecraftEvent.CHAT, MinecraftEvent.COMMAND_EXECUTED, MinecraftEvent.MESSAGE, MinecraftEvent.IDLE]})\n",
        "\n",
        "    # this is called when an event happens\n",
        "    # we return our next action\n",
        "    def on_event(self, observation):\n",
        "\n",
        "        print(observation)\n",
        "\n",
        "        print(\n",
        "            f\"Received on_event call for {self.participant_id} with event: {observation.event} task: {self.memory.task}\"\n",
        "        )\n",
        "\n",
        "        # lets convert our observation to a string, so we can pass it to the LLM\n",
        "        observation_summary = self.__format_observation(observation)\n",
        "        print(f\"\\nObservation Summary:\\n{observation_summary}\")\n",
        "        print(\"Task: \", self.memory.task)\n",
        "\n",
        "        # now we pass it to the LLM, get the next action, and send it to Kradle\n",
        "        response = self.__get_llm_response(observation_summary, observation)\n",
        "        print(f\"Agent Response: {response}\")\n",
        "        print(f\"Participant ID: {self.participant_id}\")\n",
        "\n",
        "        return response\n",
        "\n",
        "    # convert observation from object => string, so we can build a LLM prompt\n",
        "    def __format_observation(self, observation):\n",
        "\n",
        "        # python array operation to extend/append to the in-game chat history\n",
        "        self.memory.game_chat_history.extend(observation.chat_messages)\n",
        "\n",
        "        print(f\"Minecraft Chat History: {self.memory.game_chat_history}\")\n",
        "\n",
        "        # lets get everything in our inventory\n",
        "        inventory_summary = (\n",
        "            \", \".join([f\"{count} {name}\" for name, count in observation.inventory.items()])\n",
        "            if observation.inventory\n",
        "            else \"None\"\n",
        "        )\n",
        "\n",
        "        # return a string with everything the LLM needs to know\n",
        "        return (\n",
        "            f\"Event received: {observation.event if observation.event else 'None'}\\n\\n\"\n",
        "            f\"{observation.output if observation.output else 'Output: None'}\\n\\n\"\n",
        "            f\"Position: {observation.position}\\n\\n\"\n",
        "            f\"Latest Chat: {observation.chat_messages}\\n\\n\"\n",
        "            f\"Visible Players: {', '.join(observation.players) if observation.players else 'None'}\\n\\n\"\n",
        "            f\"Visible Blocks: {', '.join(observation.blocks) if observation.blocks else 'None'}\\n\\n\"\n",
        "            f\"Inventory: {inventory_summary}\\n\\n\"\n",
        "            f\"Health: {observation.health * 100}/100\"\n",
        "        )\n",
        "\n",
        "    # this function builds the system prompt for the agent\n",
        "    def __build_system_prompt(self, observation):\n",
        "\n",
        "        system_prompt = []\n",
        "        # build the system prompt for the agent\n",
        "        prompt = coding_prompt\n",
        "\n",
        "        # load task, persona, agent_modes, and commands from memory to build the prompt\n",
        "        prompt = prompt.replace(\"$NAME\", observation.name)\n",
        "        prompt = prompt.replace(\"$TASK\", self.memory.task)\n",
        "        prompt = prompt.replace(\"$AGENT_MODE\", str(self.memory.agent_modes))\n",
        "\n",
        "        if self.memory.agent_modes[\"mcmode\"] == \"creative\":\n",
        "            prompt = prompt.replace(\"$CREATIVE_MODE\", creative_mode_prompt)\n",
        "        else:\n",
        "            prompt = prompt.replace(\"$CREATIVE_MODE\", \"You are in survival mode.\")\n",
        "\n",
        "        system_prompt.append({\"role\": \"system\", \"content\": prompt})\n",
        "\n",
        "        prompt = skills_prompt\n",
        "        prompt = prompt.replace(\"$CODE_DOCS\", str(self.memory.js_functions))\n",
        "        system_prompt.append({\"role\": \"system\", \"content\": prompt})\n",
        "\n",
        "        prompt = agent_prompt\n",
        "        prompt = prompt.replace(\"$AGENT_MODE\", str(self.memory.agent_modes))\n",
        "        system_prompt.append({\"role\": \"system\", \"content\": prompt})\n",
        "\n",
        "        prompt = examples_prompt\n",
        "        prompt = prompt.replace(\"$EXAMPLES\", str(coding_examples))\n",
        "        system_prompt.append({\"role\": \"system\", \"content\": prompt})\n",
        "\n",
        "        prompt = persona_prompt\n",
        "        prompt = prompt.replace(\"$PERSONA\", self.memory.persona)\n",
        "        system_prompt.append({\"role\": \"system\", \"content\": prompt})\n",
        "\n",
        "        return system_prompt\n",
        "\n",
        "    def __truncate_prompt(self, prompt):\n",
        "        truncated_prompt = []\n",
        "        for p in prompt:\n",
        "            truncated_p = p.copy()  # Create a shallow copy of the dict\n",
        "            if len(truncated_p[\"content\"]) > 2000:\n",
        "                truncated_p[\"content\"] = truncated_p[\"content\"][:2000] + \"...\"\n",
        "            truncated_prompt.append(truncated_p)\n",
        "        return truncated_prompt\n",
        "\n",
        "    def __print_prompt(self, prompt):\n",
        "        print(\"---PROMPT---\")\n",
        "        for p in prompt:\n",
        "            print(\"--------------------------------\")\n",
        "            print(\"---\" + p[\"role\"] + \"---\")\n",
        "            print(p[\"content\"])\n",
        "        print(\"---END PROMPT---\")\n",
        "\n",
        "    # send the prompt to the LLM and get the response\n",
        "    def __get_llm_response(self, observation_summary, observation, max_retries=MAX_RETRIES):\n",
        "\n",
        "        if max_retries < MAX_RETRIES:\n",
        "            print(f\"\\033[91m########################################################\")\n",
        "            print(f\"\\033[91m########################################################\")\n",
        "            print(f\"\\033[91mRetrying LLM response for the {MAX_RETRIES - max_retries} time\\033[0m\")\n",
        "            print(f\"\\033[91m########################################################\")\n",
        "            print(f\"\\033[91m########################################################\")\n",
        "\n",
        "        # prompt = system prompt + last 5 LLM interactions + last observation\n",
        "        llm_prompt = [\n",
        "            *self.__build_system_prompt(observation),\n",
        "            *self.memory.llm_transcript[-5:],\n",
        "            {\"role\": \"user\", \"content\": observation_summary},\n",
        "        ]\n",
        "\n",
        "        json_payload = {\n",
        "            \"model\": self.memory.model,\n",
        "            \"messages\": llm_prompt,\n",
        "            \"require_parameters\": True,\n",
        "            \"response_format\": JSON_RESPONSE_FORMAT,\n",
        "        }\n",
        "\n",
        "        print(f\"using model: {self.memory.model}\")\n",
        "\n",
        "        truncated_prompt = self.__truncate_prompt(llm_prompt)\n",
        "        self.__print_prompt(truncated_prompt)\n",
        "\n",
        "        response = requests.post(\n",
        "            \"https://openrouter.ai/api/v1/chat/completions\",\n",
        "            headers={\"Authorization\": f\"Bearer {self.memory.openrouter_api_key}\"},\n",
        "            json=json_payload,\n",
        "            timeout=30,\n",
        "        ).json()\n",
        "\n",
        "        print(f\"Response: {response}\")\n",
        "\n",
        "        content, action, success, error_message = self.__process_llm_response(response)\n",
        "\n",
        "        # logging what we sent and received to the Kradle dashboard\n",
        "        self.log({\"prompt\": truncated_prompt, \"model\": self.memory.model, \"response\": content})\n",
        "\n",
        "        # append to the message history\n",
        "        self.memory.llm_transcript.extend(\n",
        "            [\n",
        "                {\"role\": \"user\", \"content\": observation_summary},\n",
        "                {\"role\": \"assistant\", \"content\": content }\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        if success:\n",
        "            return {\"code\": action[\"code\"], \"message\": action[\"message\"], \"delay\": self.memory.delay_after_action}\n",
        "\n",
        "        if max_retries <= 0:\n",
        "            return {\"code\": \"\", \"message\": \"I'm sorry, I'm having trouble generating a response. Please try again later.\", \"delay\": self.memory.delay_after_action}\n",
        "\n",
        "        self.memory.llm_transcript.append({\"role\": \"system\", \"content\": f\"your last response was not valid because: {error_message}\"})\n",
        "\n",
        "        return self.__get_llm_response(observation_summary, observation, max_retries - 1)\n",
        "\n",
        "    # process the LLM response\n",
        "    def __process_llm_response(self, response):\n",
        "        success = False\n",
        "        content = \"\"\n",
        "        action = None\n",
        "        error_message = \"\"\n",
        "        try:\n",
        "            if response[\"choices\"]:\n",
        "                content = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "                #find the first { and the last  }\n",
        "                start = content.find(\"{\")\n",
        "                end = content.rfind(\"}\") + 1\n",
        "                content_to_parse = content[start:end]\n",
        "\n",
        "\n",
        "                print(f\"Content to parse: {content_to_parse}\")\n",
        "                # Parse the content string as JSON\n",
        "                json_content = json.loads(content_to_parse)\n",
        "                action = { \"code\": json_content[\"code\"], \"message\": json_content[\"message\"] }\n",
        "                success = True\n",
        "\n",
        "            else:\n",
        "                print(f\"No response from LLM: {response}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            error_message = str(e) if e else \"\"\n",
        "\n",
        "        return content, action, success, error_message\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxcML8flNXIQ"
      },
      "source": [
        "Finally, lets start it!\n",
        "\n",
        "This will spin up a server, create your agent on Kradle, and make it available to run it in a challenge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "id": "cCuN44E185EM",
        "outputId": "715165a9-8e6f-4f25-d6c9-bc18cd7ee800"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "username: llm-agent\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Public URL </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">tunnel</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span><span style=\"color: #008000; text-decoration-color: #008000\"> created successfully</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[32m✓ Public URL \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mtunnel\u001b[0m\u001b[1;32m)\u001b[0m\u001b[32m created successfully\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">○ Agent </span><span style=\"color: #008000; text-decoration-color: #008000\">'llm-agent'</span><span style=\"color: #000080; text-decoration-color: #000080\"> not found, creating it</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[34m○ Agent \u001b[0m\u001b[32m'llm-agent'\u001b[0m\u001b[34m not found, creating it\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Agent </span><span style=\"color: #008000; text-decoration-color: #008000\">'llm-agent'</span><span style=\"color: #008000; text-decoration-color: #008000\"> created successfully</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[32m✓ Agent \u001b[0m\u001b[32m'llm-agent'\u001b[0m\u001b[32m created successfully\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">🔮 KRADLE</span> <span style=\"color: #ffffff; text-decoration-color: #ffffff\">Agent Server</span>\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1;36m🔮 KRADLE\u001b[0m \u001b[97mAgent Server\u001b[0m\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff\">AGENT   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">llm-agent</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> (Port 1500)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[97mAGENT   \u001b[0m\u001b[1;36mllm-agent\u001b[0m\u001b[2m (Port 1500)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808080; text-decoration-color: #808080\">╭──────────────────────────────────────────────────── </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Network</span><span style=\"color: #808080; text-decoration-color: #808080\"> ────────────────────────────────────────────────────╮</span>\n",
              "<span style=\"color: #808080; text-decoration-color: #808080\">│</span>                                                                                                                 <span style=\"color: #808080; text-decoration-color: #808080\">│</span>\n",
              "<span style=\"color: #808080; text-decoration-color: #808080\">│</span>                                                                                                                 <span style=\"color: #808080; text-decoration-color: #808080\">│</span>\n",
              "<span style=\"color: #808080; text-decoration-color: #808080\">│</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">→ Local URL     </span><span style=\"color: #008080; text-decoration-color: #008080\">http://localhost:1500/llm-agent</span>                                                                <span style=\"color: #808080; text-decoration-color: #808080\">│</span>\n",
              "<span style=\"color: #808080; text-decoration-color: #808080\">│</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">→ Network URL   </span><span style=\"color: #008000; text-decoration-color: #008000\">https://rnoji-34-125-191-52.a.free.pinggy.link/llm-agent</span>                                       <span style=\"color: #808080; text-decoration-color: #808080\">│</span>\n",
              "<span style=\"color: #808080; text-decoration-color: #808080\">│</span>                                                                                                                 <span style=\"color: #808080; text-decoration-color: #808080\">│</span>\n",
              "<span style=\"color: #808080; text-decoration-color: #808080\">│</span>                                                                                                                 <span style=\"color: #808080; text-decoration-color: #808080\">│</span>\n",
              "<span style=\"color: #808080; text-decoration-color: #808080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[90m╭─\u001b[0m\u001b[90m───────────────────────────────────────────────────\u001b[0m\u001b[90m \u001b[0m\u001b[37mNetwork\u001b[0m\u001b[90m \u001b[0m\u001b[90m───────────────────────────────────────────────────\u001b[0m\u001b[90m─╮\u001b[0m\n",
              "\u001b[90m│\u001b[0m                                                                                                                 \u001b[90m│\u001b[0m\n",
              "\u001b[90m│\u001b[0m                                                                                                                 \u001b[90m│\u001b[0m\n",
              "\u001b[90m│\u001b[0m  \u001b[37m→ Local URL     \u001b[0m\u001b[36mhttp://localhost:1500/llm-agent\u001b[0m                                                                \u001b[90m│\u001b[0m\n",
              "\u001b[90m│\u001b[0m  \u001b[37m→ Network URL   \u001b[0m\u001b[32mhttps://rnoji-34-125-191-52.a.free.pinggy.link/llm-agent\u001b[0m                                       \u001b[90m│\u001b[0m\n",
              "\u001b[90m│\u001b[0m                                                                                                                 \u001b[90m│\u001b[0m\n",
              "\u001b[90m│\u001b[0m                                                                                                                 \u001b[90m│\u001b[0m\n",
              "\u001b[90m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">✨ Your agent is running locally!</span>                                                                              <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>  <span style=\"color: #ffffff; text-decoration-color: #ffffff\">You can customize your agent:</span>                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">• Use any Python packages (LangChain, Transformers, etc.)</span>                                                      <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">• Integrate your own models or APIs</span>                                                                            <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">• Modify how your agent responds to events</span>                                                                     <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>  <span style=\"color: #ffffff; text-decoration-color: #ffffff\">Ready to start?</span>                                                                                                <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">→ Visit </span><span style=\"color: #008080; text-decoration-color: #008080; text-decoration: underline\">app.kradle.ai/workbench/challenges</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> to select a challenge to launch your agent into</span>                     <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>  <span style=\"color: #808000; text-decoration-color: #808000\">Keep this window open to receive game events from Kradle</span>                                                       <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[32m╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\u001b[0m\n",
              "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
              "\u001b[32m│\u001b[0m  \u001b[1;32m✨ Your agent is running locally!\u001b[0m                                                                              \u001b[32m│\u001b[0m\n",
              "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
              "\u001b[32m│\u001b[0m  \u001b[97mYou can customize your agent:\u001b[0m                                                                                  \u001b[32m│\u001b[0m\n",
              "\u001b[32m│\u001b[0m  \u001b[37m• Use any Python packages (LangChain, Transformers, etc.)\u001b[0m                                                      \u001b[32m│\u001b[0m\n",
              "\u001b[32m│\u001b[0m  \u001b[37m• Integrate your own models or APIs\u001b[0m                                                                            \u001b[32m│\u001b[0m\n",
              "\u001b[32m│\u001b[0m  \u001b[37m• Modify how your agent responds to events\u001b[0m                                                                     \u001b[32m│\u001b[0m\n",
              "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
              "\u001b[32m│\u001b[0m  \u001b[97mReady to start?\u001b[0m                                                                                                \u001b[32m│\u001b[0m\n",
              "\u001b[32m│\u001b[0m  \u001b[37m→ Visit \u001b[0m\u001b[4;36mapp.kradle.ai/workbench/challenges\u001b[0m\u001b[37m to select a challenge to launch your agent into\u001b[0m                     \u001b[32m│\u001b[0m\n",
              "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
              "\u001b[32m│\u001b[0m  \u001b[33mKeep this window open to receive game events from Kradle\u001b[0m                                                       \u001b[32m│\u001b[0m\n",
              "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
              "\u001b[32m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Press Ctrl+C to stop the server</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[2mPress Ctrl+C to stop the server\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Started agent at URL: https://rnoji-34-125-191-52.a.free.pinggy.link/llm-agent\n"
          ]
        }
      ],
      "source": [
        "# manually set the api key on the AgentManager\n",
        "# we can't rely on .env since this could be running on colab\n",
        "AgentManager.set_api_key(KRADLE_API_KEY)\n",
        "\n",
        "# finally, lets serve our agent!\n",
        "# this creates a web server and an SSH tunnel (so our agent has a stable public URL)\n",
        "connection_info = AgentManager.serve(LLMBasedAgent, create_public_url=True)\n",
        "print(f\"Started agent at URL: {connection_info}\")\n",
        "\n",
        "# now go to app.kradle.ai and run this agent against a challenge!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}